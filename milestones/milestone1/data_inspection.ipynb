{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Links\n",
    "Multilingual Text Detoxification: \n",
    "- [Task introduction page](https://pan.webis.de/clef24/pan24-web/text-detoxification.html#data)\n",
    "\n",
    "Huggingface links for the datasets:  \n",
    "- [ParaDetox datasets for English](https://huggingface.co/datasets/s-nlp/paradetox)\n",
    "- [ParaDetox datasets for Russian](https://huggingface.co/datasets/s-nlp/ru_paradetox)\n",
    "- [Multilingual ParaDetox data](https://huggingface.co/datasets/textdetox/multilingual_paradetox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the official datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the ParaDetox datasets for English and Russian, for the fine-tuning\n",
    "en_data = load_dataset('s-nlp/paradetox', split='train')\n",
    "en_train, en_valid = en_data.train_test_split(test_size=0.1, seed=42).values()\n",
    "\n",
    "ru_train = load_dataset('s-nlp/ru_paradetox', split='train')\n",
    "ru_valid = load_dataset('s-nlp/ru_paradetox', split='validation')\n",
    "\n",
    "# Load the DEV dataset - multilingual ParaDetox data\n",
    "dev_data = load_dataset('textdetox/multilingual_paradetox')\n",
    "en_dev = dev_data['en']\n",
    "ru_dev = dev_data['ru']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data samples from the offical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English dataset samples:\n",
      "{'en_toxic_comment': 'mostly because they put the fucking stadium out so far no one wants to drive there .', 'en_neutral_comment': 'mostly because they put the stadium out so far no one wants to drive there .'}\n",
      "{'en_toxic_comment': 'botch is my new word for when a boy is being a bitch', 'en_neutral_comment': 'botch is my new word for when a boy is being bad'}\n",
      "{'en_toxic_comment': 'man will you get some help for this brain washed liberal bs you live in', 'en_neutral_comment': 'Man will you get some help for this man bs you live in.'}\n",
      "{'en_toxic_comment': \"well it came fresh out his ass , so i don 't see how it could be wrong .\", 'en_neutral_comment': \"well it came fresh out his mouth , so i don 't see how it could be wrong .\"}\n",
      "{'en_toxic_comment': 'their hard drives are shit .', 'en_neutral_comment': 'Their hard drives are not good.'}\n",
      "Russian dataset samples:\n",
      "{'ru_toxic_comment': 'и,чё,блядь где этот херой был до этого со своими доказательствами?', 'ru_neutral_comment': 'Ну и где этот герой был,со своими доказательствами?'}\n",
      "{'ru_toxic_comment': 'и,чё,блядь где этот херой был до этого со своими доказательствами?', 'ru_neutral_comment': 'Где этот герой был до этого со своими доказательствами?'}\n",
      "{'ru_toxic_comment': 'и,чё,блядь где этот херой был до этого со своими доказательствами?', 'ru_neutral_comment': 'и,где этот герой был до этого со своими доказательствами?'}\n",
      "{'ru_toxic_comment': 'О, а есть деанон этого петуха?', 'ru_neutral_comment': 'О, а есть деанон'}\n",
      "{'ru_toxic_comment': 'херну всякую пишут,из-за этого лайка.долбоебизм.', 'ru_neutral_comment': 'Чушь всякую пишут, из- за этого лайка.'}\n"
     ]
    }
   ],
   "source": [
    "# English dataset samples\n",
    "print(\"English dataset samples:\")\n",
    "for i in range(5):\n",
    "    print(en_train[i])\n",
    "\n",
    "# Russian dataset samples\n",
    "print(\"Russian dataset samples:\")\n",
    "for i in range(5):\n",
    "    print(ru_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English dev dataset samples:\n",
      "{'toxic_sentence': 'then all of a sudden i see her , shes now got the big phony tits and everything .'}\n",
      "{'toxic_sentence': 'Prove my comment wrong and post a video of how fucking tough you are.'}\n",
      "{'toxic_sentence': \"My page should be protected first so that worthless scum like you can't keep vandalizing it.\"}\n",
      "{'toxic_sentence': 'i never uttered or wrote that crap .'}\n",
      "{'toxic_sentence': 'i go there for work every few months , and i can assure you it fucking sucks .'}\n",
      "Russian dev dataset samples:\n",
      "{'toxic_sentence': 'пидоры когда будем дышать чистым воздухом. адмистрация это к вам'}\n",
      "{'toxic_sentence': 'твоё очко уходит в пользу форума!'}\n",
      "{'toxic_sentence': 'и нехуя здесь орать что нам что то принадлежало'}\n",
      "{'toxic_sentence': 'еще блядь один герой нашего времени..'}\n",
      "{'toxic_sentence': 'скорее всего мы видим самый конец астронавтики пиндосии, как и самого государства извращенца.'}\n"
     ]
    }
   ],
   "source": [
    "# English dev dataset samples\n",
    "print(\"English dev dataset samples:\")\n",
    "for i in range(5):\n",
    "    print(en_dev[i])\n",
    "\n",
    "# Russian dev dataset samples\n",
    "print(\"Russian dev dataset samples:\")\n",
    "for i in range(5):\n",
    "    print(ru_dev[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_text_statistics(dataset, feature):\n",
    "    lengths = [len(sample[feature].split()) for sample in dataset]\n",
    "    return {\n",
    "        'Mean Length': sum(lengths) / len(lengths),\n",
    "        'Max Length': max(lengths),\n",
    "        'Min Length': min(lengths)\n",
    "    }\n",
    "\n",
    "# Dictionary to hold all the datasets and their corresponding features for analysis\n",
    "datasets_info = {\n",
    "    'en_train_toxic': {'dataset': en_train, 'feature': 'en_toxic_comment'},\n",
    "    'en_train_neutral': {'dataset': en_train, 'feature': 'en_neutral_comment'},\n",
    "    'en_valid_toxic': {'dataset': en_valid, 'feature': 'en_toxic_comment'},\n",
    "    'en_valid_neutral': {'dataset': en_valid, 'feature': 'en_neutral_comment'},\n",
    "    'en_dev_toxic': {'dataset': en_dev, 'feature': 'toxic_sentence'},\n",
    "    'ru_train_toxic': {'dataset': ru_train, 'feature': 'ru_toxic_comment'},\n",
    "    'ru_train_neutral': {'dataset': ru_train, 'feature': 'ru_neutral_comment'},\n",
    "    'ru_valid_toxic': {'dataset': ru_valid, 'feature': 'ru_toxic_comment'},\n",
    "    'ru_valid_neutral': {'dataset': ru_valid, 'feature': 'ru_neutral_comment'},\n",
    "    'ru_dev_toxic': {'dataset': ru_dev, 'feature': 'toxic_sentence'}\n",
    "}\n",
    "\n",
    "stats_list = []\n",
    "\n",
    "# Calculate statistics\n",
    "for name, info in datasets_info.items():\n",
    "    language, split_type = name.split('_')[0], '_'.join(name.split('_')[1:])\n",
    "    stats = {\n",
    "        'Language': language.capitalize(),\n",
    "        'Split': split_type,\n",
    "        'Number of Samples': len(info['dataset']),\n",
    "        **calculate_text_statistics(info['dataset'], info['feature'])\n",
    "    }\n",
    "    stats_list.append(stats)\n",
    "\n",
    "stats_df = pd.DataFrame(stats_list)\n",
    "\n",
    "# stats_df.to_csv('dataset_statistics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Split</th>\n",
       "      <th>Number of Samples</th>\n",
       "      <th>Mean Length</th>\n",
       "      <th>Max Length</th>\n",
       "      <th>Min Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>En</td>\n",
       "      <td>train_toxic</td>\n",
       "      <td>17769</td>\n",
       "      <td>11.851427</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>En</td>\n",
       "      <td>train_neutral</td>\n",
       "      <td>17769</td>\n",
       "      <td>9.303225</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>En</td>\n",
       "      <td>valid_toxic</td>\n",
       "      <td>1975</td>\n",
       "      <td>12.000506</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>En</td>\n",
       "      <td>valid_neutral</td>\n",
       "      <td>1975</td>\n",
       "      <td>9.442025</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>En</td>\n",
       "      <td>dev_toxic</td>\n",
       "      <td>400</td>\n",
       "      <td>11.957500</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ru</td>\n",
       "      <td>train_toxic</td>\n",
       "      <td>11090</td>\n",
       "      <td>10.371235</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ru</td>\n",
       "      <td>train_neutral</td>\n",
       "      <td>11090</td>\n",
       "      <td>8.926150</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ru</td>\n",
       "      <td>valid_toxic</td>\n",
       "      <td>1116</td>\n",
       "      <td>10.336918</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ru</td>\n",
       "      <td>valid_neutral</td>\n",
       "      <td>1116</td>\n",
       "      <td>8.660394</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ru</td>\n",
       "      <td>dev_toxic</td>\n",
       "      <td>400</td>\n",
       "      <td>10.492500</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Language          Split  Number of Samples  Mean Length  Max Length  \\\n",
       "0       En    train_toxic              17769    11.851427          20   \n",
       "1       En  train_neutral              17769     9.303225          27   \n",
       "2       En    valid_toxic               1975    12.000506          20   \n",
       "3       En  valid_neutral               1975     9.442025          23   \n",
       "4       En      dev_toxic                400    11.957500          24   \n",
       "5       Ru    train_toxic              11090    10.371235          28   \n",
       "6       Ru  train_neutral              11090     8.926150          29   \n",
       "7       Ru    valid_toxic               1116    10.336918          20   \n",
       "8       Ru  valid_neutral               1116     8.660394          24   \n",
       "9       Ru      dev_toxic                400    10.492500          25   \n",
       "\n",
       "   Min Length  \n",
       "0           1  \n",
       "1           1  \n",
       "2           5  \n",
       "3           1  \n",
       "4           4  \n",
       "5           1  \n",
       "6           1  \n",
       "7           5  \n",
       "8           1  \n",
       "9           4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
