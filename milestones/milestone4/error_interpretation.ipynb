{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'en-dev0',\n",
       "  'original_text': 'then all of a sudden i see her , shes now got the big phony tits and everything .',\n",
       "  'modified_text': 'then all of a sudden i see her , shes now got the big phony tits and everything .'},\n",
       " {'id': 'en-dev1',\n",
       "  'original_text': 'Prove my comment wrong and post a video of how fucking tough you are.',\n",
       "  'modified_text': 'prove my comment wrong and post a video of how are tough you are .'},\n",
       " {'id': 'en-dev2',\n",
       "  'original_text': \"My page should be protected first so that worthless scum like you can't keep vandalizing it.\",\n",
       "  'modified_text': \"my page should be protected first so that worthless scum like you can ' t keep vandalizing it .\"},\n",
       " {'id': 'en-dev3',\n",
       "  'original_text': 'i never uttered or wrote that crap .',\n",
       "  'modified_text': 'i never uttered or wrote that . .'},\n",
       " {'id': 'en-dev4',\n",
       "  'original_text': 'i go there for work every few months , and i can assure you it fucking sucks .',\n",
       "  'modified_text': 'i go there for work every few months , and i can assure you it . . .'},\n",
       " {'id': 'en-dev5',\n",
       "  'original_text': 'and go on saddam execution talk page, before you stink up the place.',\n",
       "  'modified_text': 'and go on saddam execution talk page , before you stink up the place .'},\n",
       " {'id': 'en-dev6',\n",
       "  'original_text': 'eli5 why should governments give a fuck ? .',\n",
       "  'modified_text': 'eli5 why should governments give a . ? .'},\n",
       " {'id': 'en-dev7',\n",
       "  'original_text': 'that stupid water pipe is just a fatal accident waiting to happen .',\n",
       "  'modified_text': 'that so water pipe is just a fatal accident waiting to happen .'},\n",
       " {'id': 'en-dev8',\n",
       "  'original_text': 'not the rich and the government , but the people will be in shit street .',\n",
       "  'modified_text': 'not the rich and the government , but the people will be in streets street .'},\n",
       " {'id': 'en-dev9',\n",
       "  'original_text': 'why do you guys do this shit out west ?',\n",
       "  'modified_text': 'why do you guys do this , out west ?'},\n",
       " {'id': 'en-dev10',\n",
       "  'original_text': 'not just a biologist , a fucking virologist ! .',\n",
       "  'modified_text': 'not just a biologist , a a virologist ! .'},\n",
       " {'id': 'en-dev11',\n",
       "  'original_text': \"that things that we use everyday aren't correct. wikipedia blows cock and so do you.\",\n",
       "  'modified_text': \"that things that we use everyday aren ' t correct . wikipedia blows cock and so do you .\"},\n",
       " {'id': 'en-dev12',\n",
       "  'original_text': 'ps, congradulations on recieving the bitch star, you definatly deserve it ryan.',\n",
       "  'modified_text': 'ps , congradulations on recieving the , star , you definatly deserve it ryan .'},\n",
       " {'id': 'en-dev13',\n",
       "  'original_text': 'my comment was more a diatribe about how there should be more outrage at home for this shit .',\n",
       "  'modified_text': 'my comment was more a diatribe about how there should be more outrage at home for this . .'},\n",
       " {'id': 'en-dev14',\n",
       "  'original_text': 'DIGIT year olds can be little shits too , doesn t mean you fight them .',\n",
       "  'modified_text': 'digit year olds can be people ands too , doesn t mean you fight them .'},\n",
       " {'id': 'en-dev15',\n",
       "  'original_text': 'and because the government need this land , fuck people who live there , right ? .',\n",
       "  'modified_text': 'and because the government need this land , people people who live there , right ? .'},\n",
       " {'id': 'en-dev16',\n",
       "  'original_text': 'stupid for a ball coach to make ten times what a teacher makes .',\n",
       "  'modified_text': 'the for a ball coach to make ten times what a teacher makes .'},\n",
       " {'id': 'en-dev17',\n",
       "  'original_text': 'priceless, I hate that guy',\n",
       "  'modified_text': 'priceless , i hate that guy'},\n",
       " {'id': 'en-dev18',\n",
       "  'original_text': '&gt today was one of the most fucked up days of my life .',\n",
       "  'modified_text': '& gt today was one of the most white up days of my life .'},\n",
       " {'id': 'en-dev19',\n",
       "  'original_text': 'Next time, I will force you to eat your own shit.',\n",
       "  'modified_text': 'next time , i will force you to eat your own . .'},\n",
       " {'id': 'en-dev20',\n",
       "  'original_text': 'Oh, that was my dumbass mistake.',\n",
       "  'modified_text': 'oh , that was my !ass mistake .'},\n",
       " {'id': 'en-dev21',\n",
       "  'original_text': 'the woman s a bitch , but the man s a psychopath .',\n",
       "  'modified_text': 'the woman s a to , but the man s a psychopath .'},\n",
       " {'id': 'en-dev22',\n",
       "  'original_text': 'just a thought , but maybe stay the fuck away from the chinese .',\n",
       "  'modified_text': 'just a thought , but maybe stay the ivory away from the chinese .'},\n",
       " {'id': 'en-dev23',\n",
       "  'original_text': 'or just go with this cunt en .',\n",
       "  'modified_text': 'or just go with this cunt en .'},\n",
       " {'id': 'en-dev24',\n",
       "  'original_text': 'Nazi Sean Hoyland vandalized these pages.',\n",
       "  'modified_text': 'nazi sean hoyland vandalized these pages .'},\n",
       " {'id': 'en-dev25',\n",
       "  'original_text': \"I'm not going to reply to your ignorants comments, it's a zero!\",\n",
       "  'modified_text': \"i ' m not going to reply to your commentss comments , it ' s a zero !\"},\n",
       " {'id': 'en-dev26',\n",
       "  'original_text': 'these idiots are bound and determined to start a civil war !',\n",
       "  'modified_text': 'these and are bound and determined to start a civil war !'},\n",
       " {'id': 'en-dev27',\n",
       "  'original_text': 'abortion is not a fundamental right it is a proceedure , nothing more idiot',\n",
       "  'modified_text': 'abortion is not a fundamental right it is a proceedure , nothing more .'},\n",
       " {'id': 'en-dev28',\n",
       "  'original_text': 'his hypocrisy knowns no bounds .',\n",
       "  'modified_text': 'his hypocrisy knowns no bounds .'},\n",
       " {'id': 'en-dev29',\n",
       "  'original_text': 'what a bunch of sick lies !',\n",
       "  'modified_text': 'what a bunch of sick lies !'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "original = '/Users/SFL/Documents/GitHub/COLX_531_speech_sanitizers/data/en_dev.jsonl'\n",
    "model = '/Users/SFL/Documents/GitHub/COLX_531_speech_sanitizers/milestone5/en_dev_condbert_output.jsonl'\n",
    "\n",
    "original_sentences = {}\n",
    "with open(original, 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        original_sentences[data['id']] = data['text']\n",
    "\n",
    "model_sentences = {}\n",
    "with open(model, 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        model_sentences[data['id']] = data['text']\n",
    "\n",
    "# Combining the original and modified sentences with the same ID\n",
    "combined_sentences = []\n",
    "for id, original_text in original_sentences.items():\n",
    "    if id in model_sentences:\n",
    "        combined_sentences.append({\n",
    "            'id': id,\n",
    "            'original_text': original_text,\n",
    "            'modified_text': model_sentences[id]\n",
    "        })\n",
    "\n",
    "# Displaying a few combined entries for verification\n",
    "combined_sentences[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'ru-dev0',\n",
       "  'original_text': 'пидоры когда будем дышать чистым воздухом. адмистрация это к вам',\n",
       "  'modified_text': 'пидоры когда будем дышать чистым воздухом . адмистрация это к вам'},\n",
       " {'id': 'ru-dev1',\n",
       "  'original_text': 'твоё очко уходит в пользу форума!',\n",
       "  'modified_text': 'твоё очко уходит в пользу форума !'},\n",
       " {'id': 'ru-dev2',\n",
       "  'original_text': 'и нехуя здесь орать что нам что то принадлежало',\n",
       "  'modified_text': 'и нехуя здесь орать что нам что то принадлежало'},\n",
       " {'id': 'ru-dev3',\n",
       "  'original_text': 'еще блядь один герой нашего времени..',\n",
       "  'modified_text': 'еще !дь один герой нашего времени . .'},\n",
       " {'id': 'ru-dev4',\n",
       "  'original_text': 'скорее всего мы видим самый конец астронавтики пиндосии, как и самого государства извращенца.',\n",
       "  'modified_text': 'скорее всего мы видим самый конец астронавтики пиндосии , как и самого государства извращенца .'},\n",
       " {'id': 'ru-dev5',\n",
       "  'original_text': 'дурак ,это просто кошмар а не массаж',\n",
       "  'modified_text': '.к , это просто кошмар а не массаж'},\n",
       " {'id': 'ru-dev6',\n",
       "  'original_text': 'я из америки, российская ты шваль',\n",
       "  'modified_text': 'я из америки , российская ты шваль'},\n",
       " {'id': 'ru-dev7',\n",
       "  'original_text': 'и опять, метлы, помидоры и подарки. снова теплицы, столярку, котлызаепали',\n",
       "  'modified_text': 'и опять , метлы , помидоры и подарки . снова теплицы , столярку , котлызаепали'},\n",
       " {'id': 'ru-dev8',\n",
       "  'original_text': 'Пиздоболом является владелец сайта на котором нарушаются Правила этого сайта публично выставленные',\n",
       "  'modified_text': 'Пиздоболом является владелец сайта на котором нарушаются Правила этого сайта публично выставленные'},\n",
       " {'id': 'ru-dev9',\n",
       "  'original_text': 'барса говно спартак бы забыль бы 6:0',\n",
       "  'modified_text': 'барса говно спартак бы забыль бы 6 : 0'},\n",
       " {'id': 'ru-dev10',\n",
       "  'original_text': 'А нахрена тогда ты здесь это писал?',\n",
       "  'modified_text': 'А нахрена тогда ты здесь это писал ?'},\n",
       " {'id': 'ru-dev11',\n",
       "  'original_text': 'очередное разводилово. дураки уже переводятся, все это пиздешь.',\n",
       "  'modified_text': 'очередное разводилово . ,ки уже переводятся , все это пиздешь .'},\n",
       " {'id': 'ru-dev12',\n",
       "  'original_text': 'Ну вот значит что если ты не скидываешь бумагу в унитаз то ты отдаешь свои материалы на изучение пиндосам.',\n",
       "  'modified_text': 'Ну вот значит что если ты не скидываешь бумагу в унитаз то ты отдаешь свои материалы на изучение пиндосам .'},\n",
       " {'id': 'ru-dev13',\n",
       "  'original_text': 'И хули в золотом альбоме нет трека блек фуфела? Незаслуженно забытый трек.',\n",
       "  'modified_text': 'И , в золотом альбоме нет трека ?ек фуфела ? Незаслуженно забытый трек .'},\n",
       " {'id': 'ru-dev14',\n",
       "  'original_text': 'Я знаю про охуенную грузинскую культуру (которую вы, судя по всему, нагло спиздили в свое время).',\n",
       "  'modified_text': 'Я знаю про охуенную грузинскую культуру ( которую вы , судя по всему , нагло спиздили в свое время ) .'},\n",
       " {'id': 'ru-dev15',\n",
       "  'original_text': 'любой человек гнида редкостная , и не важно хорошо нам или плохо .',\n",
       "  'modified_text': 'любой человек гнида редкостная , и не важно хорошо нам или плохо .'},\n",
       " {'id': 'ru-dev16',\n",
       "  'original_text': 'два пидора судьям мозги ебут. а те их слушают.',\n",
       "  'modified_text': 'два пидора судьям мозги ебут . а те их слушают .'},\n",
       " {'id': 'ru-dev17',\n",
       "  'original_text': 'пидрило, не приписывай другим свои памятные достижения!!! уёбище подмосковное',\n",
       "  'modified_text': 'пидрило , не приписывай другим свои памятные достижения ! ! ! уёбище подмосковное'},\n",
       " {'id': 'ru-dev18',\n",
       "  'original_text': 'бля, при чем тут ваши шишки?',\n",
       "  'modified_text': '. , при чем тут ваши шишки ?'},\n",
       " {'id': 'ru-dev19',\n",
       "  'original_text': 'а тебя это с какой стороны колышет, уёбище?',\n",
       "  'modified_text': 'а тебя это с какой стороны колышет , уёбище ?'},\n",
       " {'id': 'ru-dev20',\n",
       "  'original_text': 'сучка ты драная, сидишь в своей вонючей латвии и сиди. нечего свой поганый нос сунуть в чужую страну.',\n",
       "  'modified_text': 'сучка ты драная , сидишь в своей вонючей латвии и сиди . нечего свой поганый нос сунуть в чужую страну .'},\n",
       " {'id': 'ru-dev21',\n",
       "  'original_text': 'я в шоке неужели вот эта прости господи пизда не чувствует что у неё разрез по самую не хочу',\n",
       "  'modified_text': 'я в шоке неужели вот эта прости господи пизда не чувствует что у неё разрез по самую не хочу'},\n",
       " {'id': 'ru-dev22',\n",
       "  'original_text': 'лично я сейчас за дальневосточную республику,как 1918-1922г,сибирь,алтай,и др.проживут без москалей и пидорцев,а вот они?',\n",
       "  'modified_text': 'лично я сейчас за дальневосточную республику , как 1918 - 1922г , сибирь , алтай , и др . проживут без москалей и пидорцев , а вот они ?'},\n",
       " {'id': 'ru-dev23',\n",
       "  'original_text': 'а ты,что 41 год помнишь? сколько этой шлюхе денег заплатили,чтоб она такую чушь несла.',\n",
       "  'modified_text': 'а ты , что 41 год помнишь ? сколько этой шлюхе денег заплатили , чтоб она такую чушь несла .'},\n",
       " {'id': 'ru-dev24',\n",
       "  'original_text': 'яб сказал:хорошо что ты водила маршрутки,блядь!',\n",
       "  'modified_text': 'яб сказал : хорошо что ты водила маршрутки , .дь !'},\n",
       " {'id': 'ru-dev25',\n",
       "  'original_text': 'начни со сваих выблядко в',\n",
       "  'modified_text': 'начни со сваих выблядко в'},\n",
       " {'id': 'ru-dev26',\n",
       "  'original_text': 'вас три дуры в этом обсуждении верят в ковид!',\n",
       "  'modified_text': 'вас три дуры в этом обсуждении верят в ковид !'},\n",
       " {'id': 'ru-dev27',\n",
       "  'original_text': 'чтобы ещё раз все убедились, в творческой среде пидоросятина в порядке вещей.',\n",
       "  'modified_text': 'чтобы ещё раз все убедились , в творческой среде пидоросятина в порядке вещей .'},\n",
       " {'id': 'ru-dev28',\n",
       "  'original_text': 'ненавижу презираю никогда врагам не верила и не доверяла чтоб он издох !!! чмо ботоксное',\n",
       "  'modified_text': 'ненавижу презираю никогда врагам не верила и не доверяла чтоб он издох ! ! ! чмо ботоксное'},\n",
       " {'id': 'ru-dev29',\n",
       "  'original_text': 'Можно хоть один тред без политоты? Даже тебя хуеосить не хочется, давай лучше про няшек-фигуристок говорить.',\n",
       "  'modified_text': 'Можно хоть один тред без политоты ? Даже тебя хуеосить не хочется , давай лучше про няшек - фигуристок говорить .'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "original = '/Users/SFL/Documents/GitHub/COLX_531_speech_sanitizers/data/ru_dev.jsonl'\n",
    "model = '/Users/SFL/Documents/GitHub/COLX_531_speech_sanitizers/milestone5/ru_dev_condbert_output.jsonl'\n",
    "\n",
    "original_sentences = {}\n",
    "with open(original, 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        original_sentences[data['id']] = data['text']\n",
    "\n",
    "model_sentences = {}\n",
    "with open(model, 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        model_sentences[data['id']] = data['text']\n",
    "\n",
    "# Combining the original and modified sentences with the same ID\n",
    "combined_sentences = []\n",
    "for id, original_text in original_sentences.items():\n",
    "    if id in model_sentences:\n",
    "        combined_sentences.append({\n",
    "            'id': id,\n",
    "            'original_text': original_text,\n",
    "            'modified_text': model_sentences[id]\n",
    "        })\n",
    "\n",
    "# Displaying a few combined entries for verification\n",
    "combined_sentences[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import json\n",
    "\n",
    "# original = '/Users/SFL/Documents/GitHub/COLX_531_speech_sanitizers/data/en_valid_input.jsonl'\n",
    "# model = '/Users/SFL/Documents/GitHub/COLX_531_speech_sanitizers/milestone5/en_dev_condbert_output.jsonl'\n",
    "# gold = '/Users/SFL/Documents/GitHub/COLX_531_speech_sanitizers/data/en_valid_gold.jsonl'\n",
    "\n",
    "# original_sentences = {}\n",
    "# with open(original, 'r') as file:\n",
    "#     for line in file:\n",
    "#         data = json.loads(line)\n",
    "#         original_sentences[data['id']] = data['text']\n",
    "\n",
    "# model_sentences = {}\n",
    "# with open(model, 'r') as file:\n",
    "#     for line in file:\n",
    "#         data = json.loads(line)\n",
    "#         model_sentences[data['id']] = data['text']\n",
    "\n",
    "# gold_sentences = {}\n",
    "# with open(gold, 'r') as file:\n",
    "#     for line in file:\n",
    "#         data = json.loads(line)\n",
    "#         gold_sentences[data['id']] = data['text']\n",
    "\n",
    "# # Combining the original and modified sentences with the same ID\n",
    "# combined_sentences = []\n",
    "# for id, original_text in original_sentences.items():\n",
    "#     if id in model_sentences and id in gold_sentences:\n",
    "#         combined_sentences.append({\n",
    "#             'id': id,\n",
    "#             'original_text': original_text,\n",
    "#             'modified_text': model_sentences[id],\n",
    "#             'gold_____text': gold_sentences[id]\n",
    "#         })\n",
    "\n",
    "# # Displaying a few combined entries for verification\n",
    "# combined_sentences[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_file_path = '/Users/SFL/Documents/GitHub/COLX_531_speech_sanitizers/data/en_dev.jsonl'\n",
    "# /Users/SFL/Documents/GitHub/COLX_531_speech_sanitizers/data/ru_dev.jsonl\n",
    "modified_file_path = '/Users/SFL/Documents/GitHub/COLX_531_speech_sanitizers/milestone3/en_dev_output_finetuned-bart-f1.jsonl'\n",
    "# /Users/SFL/Documents/GitHub/COLX_531_speech_sanitizers/milestone3/ru_dev_output_finetuned-bart-f2.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined sentences have been exported to /Users/SFL/Downloads/en.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def combine_and_export_sentences(original_file_path, modified_file_path, output_file_path):\n",
    "    # Load original sentences\n",
    "    original_sentences = {}\n",
    "    with open(original_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            original_sentences[data['id']] = data['text']\n",
    "    \n",
    "    # Load modified sentences\n",
    "    modified_sentences = {}\n",
    "    with open(modified_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            modified_sentences[data['id']] = data['text']\n",
    "    \n",
    "    # Combine original and modified sentences\n",
    "    combined_sentences = []\n",
    "    for id, original_text in original_sentences.items():\n",
    "        if id in modified_sentences:\n",
    "            combined_sentences.append({\n",
    "                'id': id,\n",
    "                'original_text': original_text,\n",
    "                'modified_text': modified_sentences[id]\n",
    "            })\n",
    "    \n",
    "    # Export combined sentences to a new file\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        for sentence in combined_sentences:\n",
    "            file.write(json.dumps(sentence) + '\\n')\n",
    "\n",
    "output_file_path = '/Users/SFL/Downloads/en.jsonl'\n",
    "\n",
    "# Combine and export the sentences\n",
    "combine_and_export_sentences(original_file_path, modified_file_path, output_file_path)\n",
    "\n",
    "print(\"Combined sentences have been exported to\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
