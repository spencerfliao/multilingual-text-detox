{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'ru-dev0',\n",
       "  'original_text': 'пидоры когда будем дышать чистым воздухом. адмистрация это к вам',\n",
       "  'modified_text': 'Когда будем дышать чистым воздухом. Адмистрация это к вам'},\n",
       " {'id': 'ru-dev1',\n",
       "  'original_text': 'твоё очко уходит в пользу форума!',\n",
       "  'modified_text': 'твоё лицо уходит в пользу форума!'},\n",
       " {'id': 'ru-dev2',\n",
       "  'original_text': 'и нехуя здесь орать что нам что то принадлежало',\n",
       "  'modified_text': 'И нечего здесь кричать, что нам что то принадлежало'},\n",
       " {'id': 'ru-dev3',\n",
       "  'original_text': 'еще блядь один герой нашего времени..',\n",
       "  'modified_text': 'Ещё один герой нашего времени'},\n",
       " {'id': 'ru-dev4',\n",
       "  'original_text': 'скорее всего мы видим самый конец астронавтики пиндосии, как и самого государства извращенца.',\n",
       "  'modified_text': 'скорее всего мы видим самый конец астронавтики США, как и самого государства'},\n",
       " {'id': 'ru-dev5',\n",
       "  'original_text': 'дурак ,это просто кошмар а не массаж',\n",
       "  'modified_text': 'Это просто кошмар, а не массаж'},\n",
       " {'id': 'ru-dev6',\n",
       "  'original_text': 'я из америки, российская ты шваль',\n",
       "  'modified_text': 'я из америки, российская ты плохая'},\n",
       " {'id': 'ru-dev7',\n",
       "  'original_text': 'и опять, метлы, помидоры и подарки. снова теплицы, столярку, котлызаепали',\n",
       "  'modified_text': 'И опять метлы, помидоры и подарки. Снова теплицы, столярку, кот'},\n",
       " {'id': 'ru-dev8',\n",
       "  'original_text': 'Пиздоболом является владелец сайта на котором нарушаются Правила этого сайта публично выставленные',\n",
       "  'modified_text': 'Злоупотребителем является владелец сайта на котором нарушаются Правила этого сайта публично выставленные'},\n",
       " {'id': 'ru-dev9',\n",
       "  'original_text': 'барса говно спартак бы забыль бы 6:0',\n",
       "  'modified_text': 'барса плохой спартак бы забыль бы 6:0'},\n",
       " {'id': 'ru-dev10',\n",
       "  'original_text': 'А нахрена тогда ты здесь это писал?',\n",
       "  'modified_text': 'Зачем тогда ты это писал?'},\n",
       " {'id': 'ru-dev11',\n",
       "  'original_text': 'очередное разводилово. дураки уже переводятся, все это пиздешь.',\n",
       "  'modified_text': 'очередное разводилово. люди уже переводятся, все это враньё.'},\n",
       " {'id': 'ru-dev12',\n",
       "  'original_text': 'Ну вот значит что если ты не скидываешь бумагу в унитаз то ты отдаешь свои материалы на изучение пиндосам.',\n",
       "  'modified_text': 'Ну вот значит что если ты не скидываешь бумагу в унитаз то ты отдаешь свои'},\n",
       " {'id': 'ru-dev13',\n",
       "  'original_text': 'И хули в золотом альбоме нет трека блек фуфела? Незаслуженно забытый трек.',\n",
       "  'modified_text': 'И почему в золотом альбоме нет трека блек фуфела? Незаслуженно'},\n",
       " {'id': 'ru-dev14',\n",
       "  'original_text': 'Я знаю про охуенную грузинскую культуру (которую вы, судя по всему, нагло спиздили в свое время).',\n",
       "  'modified_text': 'Я знаю про грузинскую культуру, которую вы судя по всему, украли в свое время'},\n",
       " {'id': 'ru-dev15',\n",
       "  'original_text': 'любой человек гнида редкостная , и не важно хорошо нам или плохо .',\n",
       "  'modified_text': 'Любой человек плохой, и не важно хорошо нам или плохо.'},\n",
       " {'id': 'ru-dev16',\n",
       "  'original_text': 'два пидора судьям мозги ебут. а те их слушают.',\n",
       "  'modified_text': 'Два человека судьям мозги ломают, а те их слушают'},\n",
       " {'id': 'ru-dev17',\n",
       "  'original_text': 'пидрило, не приписывай другим свои памятные достижения!!! уёбище подмосковное',\n",
       "  'modified_text': 'Не приписывай другим свои памятные достижения!!!'},\n",
       " {'id': 'ru-dev18',\n",
       "  'original_text': 'бля, при чем тут ваши шишки?',\n",
       "  'modified_text': 'При чем тут ваши шишки?'},\n",
       " {'id': 'ru-dev19',\n",
       "  'original_text': 'а тебя это с какой стороны колышет, уёбище?',\n",
       "  'modified_text': 'А тебя это с какой стороны волнует?'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Paths to the files\n",
    "# original_file_path = '/Users/SFL/Documents/GitHub/COLX_531_speech_sanitizers/data/en_dev.jsonl'\n",
    "original_file_path = '/Users/SFL/Documents/GitHub/COLX_531_speech_sanitizers/data/ru_dev.jsonl'\n",
    "# modified_file_path = '/Users/SFL/Documents/GitHub/COLX_531_speech_sanitizers/milestone3/en_dev_output_finetuned-bart-f1.jsonl'\n",
    "modified_file_path = '/Users/SFL/Documents/GitHub/COLX_531_speech_sanitizers/milestone3/ru_dev_output_finetuned-bart-f2.jsonl'\n",
    "\n",
    "# Loading the original sentences\n",
    "original_sentences = {}\n",
    "with open(original_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        original_sentences[data['id']] = data['text']\n",
    "\n",
    "# Loading the modified sentences\n",
    "modified_sentences = {}\n",
    "with open(modified_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        modified_sentences[data['id']] = data['text']\n",
    "\n",
    "# Combining the original and modified sentences with the same ID\n",
    "combined_sentences = []\n",
    "for id, original_text in original_sentences.items():\n",
    "    if id in modified_sentences:\n",
    "        combined_sentences.append({\n",
    "            'id': id,\n",
    "            'original_text': original_text,\n",
    "            'modified_text': modified_sentences[id]\n",
    "        })\n",
    "\n",
    "# Displaying a few combined entries for verification\n",
    "combined_sentences[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_file_path = '/Users/SFL/Documents/GitHub/COLX_531_speech_sanitizers/data/en_dev.jsonl'\n",
    "# /Users/SFL/Documents/GitHub/COLX_531_speech_sanitizers/data/ru_dev.jsonl\n",
    "modified_file_path = '/Users/SFL/Documents/GitHub/COLX_531_speech_sanitizers/milestone3/en_dev_output_finetuned-bart-f1.jsonl'\n",
    "# /Users/SFL/Documents/GitHub/COLX_531_speech_sanitizers/milestone3/ru_dev_output_finetuned-bart-f2.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined sentences have been exported to /Users/SFL/Downloads/en.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def combine_and_export_sentences(original_file_path, modified_file_path, output_file_path):\n",
    "    # Load original sentences\n",
    "    original_sentences = {}\n",
    "    with open(original_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            original_sentences[data['id']] = data['text']\n",
    "    \n",
    "    # Load modified sentences\n",
    "    modified_sentences = {}\n",
    "    with open(modified_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            modified_sentences[data['id']] = data['text']\n",
    "    \n",
    "    # Combine original and modified sentences\n",
    "    combined_sentences = []\n",
    "    for id, original_text in original_sentences.items():\n",
    "        if id in modified_sentences:\n",
    "            combined_sentences.append({\n",
    "                'id': id,\n",
    "                'original_text': original_text,\n",
    "                'modified_text': modified_sentences[id]\n",
    "            })\n",
    "    \n",
    "    # Export combined sentences to a new file\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        for sentence in combined_sentences:\n",
    "            file.write(json.dumps(sentence) + '\\n')\n",
    "\n",
    "output_file_path = '/Users/SFL/Downloads/en.jsonl'\n",
    "\n",
    "# Combine and export the sentences\n",
    "combine_and_export_sentences(original_file_path, modified_file_path, output_file_path)\n",
    "\n",
    "print(\"Combined sentences have been exported to\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
